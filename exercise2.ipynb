{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's now look at an implementation of a neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from graphviz import Digraph\n",
    "import math\n",
    "\n",
    "# Reusing the draw function from the previous notebook\n",
    "def trace(root):\n",
    "  # builds a set of all nodes and edges in the graph\n",
    "  nodes, edges = set(), set()\n",
    "  def build(v):\n",
    "    if v not in nodes:\n",
    "      nodes.add(v)\n",
    "      for child in v._prev:\n",
    "        edges.add((child, v))\n",
    "        build(child)\n",
    "  build(root)\n",
    "  return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "  dot = Digraph(format='svg', graph_attr={'rankdir':'LR'}) # Left to right graph\n",
    "  nodes, edges = trace(root)\n",
    "  for n in nodes:\n",
    "    uid = str(id(n))\n",
    "    dot.node(name=uid, label= \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape='record')  # added grad to the node \n",
    "\n",
    "    if n._op: # will work for non empty _op for every node, e.g for e, d, L \n",
    "      dot.node(name=uid + n._op, label= n._op ) # creating a node for the operation\n",
    "      dot.edge(uid + n._op, uid) # connecting the operation (oval) node to the Value [rectangle] node\n",
    "\n",
    "  for n1, n2 in edges:\n",
    "    dot.edge(str(id(n1)), str(id(n2)) + n2._op) # connecting the parent to the child with the operation, if there is any\n",
    "\n",
    "  return dot\n",
    "\n",
    "\n",
    "# Reusing the value class from the previous notebook\n",
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op='', label=''): # _op is empty for leaves\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None # empty function by default to calculate the gradient of the node\n",
    "        self._prev = set(_children) \n",
    "        self._op = _op \n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self): \n",
    "        return f\"Value(data={self.data})\"\n",
    "    \n",
    "    def __add__(self, other): \n",
    "        out = Value(self.data + other.data, (self, other), '+') \n",
    "        # we want to take out's grad and propagate it to self's grad and other's grad\n",
    "        # in general, we multiply the local derivative with the derivative which has propagated\n",
    "        # from the last (rightmost) node\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad # 1 is the local derivative of the addition operation\n",
    "            other.grad += 1.0 * out.grad # out.grad is the derivative propagated from the last node\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)  # if other is not a Value object, convert it to one\n",
    "        out = Value(self.data * other.data, (self, other), '*') \n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad # other.data is the local derivative of the multiplication operation\n",
    "            other.grad += self.data * out.grad # self.data is the local derivative of the multiplication operation\n",
    "            # out.grad is the derivative propagated from the last node\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self.__mul__(other)  \n",
    "\n",
    "    def tanh(self): \n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n",
    "        out = Value(t, (self,), 'tanh') \n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 - t**2) * out.grad # 1 - tanh^2(x) is the local derivative of tanh, out.grad is the derivative propagated from the last node\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        self.grad = 1.0\n",
    "        for v in reversed(topo):\n",
    "            v._backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Again let's look at the diagram of a mathematical neuron object\n",
    "\n",
    "![neuron](./neuron.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The task of a single neuron is to do multiply and accumulate. In other words, it's doing a vector dot product between two vectors. One is the input $x$ and the other is the weights $w$. Then it passes that through an activation function $(\\sigma)$ to introduce non linearity.\n",
    "\n",
    "In other words, if we consider a neuron to be a function $f(x)$, $$f(x) = \\sigma(<w|x> + b)= \\sigma(\\sum_{i}^{n}w_i\\cdot x_i + b)$$\n",
    "\n",
    "\n",
    "Let's now look at it in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.9867920106715578)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Neuron:\n",
    "\n",
    "  def __init__ (self, nin): # nin is the number of inputs to the neuron\n",
    "    self.w = [Value(random.uniform(-1,1)) for _ in range(nin)] # weights\n",
    "    self.b = Value(random.uniform(-1,1))\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # w*x + b\n",
    "    act = sum([wi*xi for wi, xi in zip(self.w, x)], self.b)\n",
    "    out = act.tanh()\n",
    "    return out\n",
    "  \n",
    "x = [2.0, 5.0]\n",
    "n = Neuron(2)\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great. Now we want to build a neural network. \n",
    "\n",
    "A neural network is an example of an $n-\\text{partite}$ graph. Neurons are connected to each other in layers. And no two neurons from the same layer have a connection between them. Generally all the neurons from one level are connected to all the other neurons in the next level (though in practice we sometimes delete edges between neurons)\n",
    "\n",
    "\n",
    "![neural-network](./neural-network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's now introduce a layer class which implements the collection on neurons abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=-0.9989084311917241),\n",
       " Value(data=0.7335905736362098),\n",
       " Value(data=-0.987596485248243)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Layer:\n",
    "\n",
    "  def __init__(self, nin, nout):\n",
    "    self.neurons = [Neuron(nin) for _ in range(nout)] # number of neurons in the layer\n",
    "\n",
    "  def __call__(self, x):\n",
    "    outs = [n(x) for n in self.neurons]\n",
    "    return outs[0] if len(outs) == 1 else outs\n",
    "  \n",
    "x = [3.0, 5.0]\n",
    "l = Layer(2, 3)\n",
    "l(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, let's build a multi layer perceptron using the classes defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "  def __init__(self, nin, nouts): # list of number of neurons in each layer\n",
    "    sz = [nin] + nouts\n",
    "    self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(sz)-1)] # creating the layers\n",
    "\n",
    "  def __call__(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"./neural_net2.jpeg\" width=\"500\"/>\n",
    "</div>\n",
    "<h3>Language models are statistical models which predict what text to generate based on a given seed text</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.906060695088756)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## Implementing the above diagram in code\n",
    "x = [3.0, 5.0, -2.0] # 3 inputs\n",
    "n = MLP(3, [4, 4, 1]) # 3 inputs, two hidden layers with 4 neurons each, 1 output\n",
    "n(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The diagram of the expression graph of this whole MLP would be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_dot(n(x)) # uncomment to see the diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
